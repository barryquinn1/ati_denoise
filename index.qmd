---
title: "Denoising in Finance"
author: "Barry Quinn"
date: "`r Sys.Date()`"
css: mycssblend.css
logo: img/qbslogo.png
title-slide-attributes:
  data-background-image: img/title-slide-img.png
  data-background-size: contain
  data-background-opacity: "0.5"
footer: "AI and Trading"
execute: 
  echo: true
format: 
  revealjs:
    scrollable: true
    slide-number: c/t
---

```{r setup, include=FALSE}
library(tidyverse)
library(babynames)
library(fontawesome) 
library(DiagrammeR)
library(RefManageR)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "alphabetic",
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("refs.bib", check = FALSE)
source("fml_fns.R")
```

## Learning Outcomes


### Matrix Algebra Fundamentals
- Eigenvalues and Eigenvectors
- Marcenko-Pastur Theorem

### Application in Finance
- Denoising Financial Covariance Matrices
- Impact on Investment Algorithms

## Why Study Denoising? {.smaller}

- Covariance matrices play a critical role in various financial analyses, including estimating risks, optimizing portfolios, simulating scenarios, clustering, and reducing dimensions. 
- Accurately estimating the covariance matrix is vital for reliable financial modeling.

### Key Term: Noise

- Noisy data arises when empirically determined covariance matrices suffer from imperfect or limited observations, rendering subsequent calculations less useful. 
- Therefore, preprocessing the data to remove noise before performing other tasks becomes necessary.

## Benefits of Denoising {.fatinline}

- More accurate risk assessment
- Robust portfolio construction
- Better simulation and prediction


## Eigen.....

#### The story of the billion dollar eigenvector

-   Larry page and Sergy Bin built a billion dollar company based on solving a linear alegbra problem using eigenvectors
-   In the following clip, Professor Steve Strogatz tells the story of the eigenvector, and its billion dollar application in Google.

[Soundcloud clip](https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/302257938&color=%23d0aca4&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true&visual=true)

## Core Insights in Financial Matrices {.smaller}

#### **Eigenvalues and Eigenvectors: The Essence of Big Data in Finance**

- The shift towards Big Data is reshaping finance, emphasizing the need for data-driven insights.
- Data are increasingly structured in matrices, central to modern financial analysis.
- Success in quantitative finance now hinges on the ability to decode complex data matrices and convey their essence.
- At the heart of these matrices are eigenvalues and eigenvectors, key to unlocking clear, actionable insights from within the data maze.
- Understanding these concepts is crucial for navigating the data-centric landscape of contemporary finance.

## Deep dive

::: columns
::: {.column}
![](img/vectors.png)
:::

::: {.column .small}
#### Simplifying Matrix Concepts: The 2D Vector
- A foundational element in matrix theory, the two-dimensional vector, embodies simplicity.
- Comprising just two components, it directly maps to coordinates on a 2D plane.
- This fundamental structure serves as a practical example of how matrices relate to geometric spaces.
:::
:::

## Deep dive

::: columns
::: {.column}

![](img/lineartransform.png)
:::
::: {.column .small}
### Linear Transformation Essentials

- Linear transformations adjust vectors through matrix multiplication, stretching or compressing them along defined axes.
- The matrix $\begin{bmatrix} 3 & 1 \\ 1 & 2 \end{bmatrix}$ shifts the x-axis to align with $\begin{bmatrix} 3&1 \end{bmatrix}$ and the y-axis with $\begin{bmatrix} 1&2 \end{bmatrix}$.
- These shifts are visualized by color-coded lines, showing how the transformation alters the coordinate system's orientation.
:::
:::

## Deep dive

::: columns
::: {.column}
![](img/lineartransform2.png)
:::
::: {.column .small}
#### Eigenvectors: Stability in Transformation

- Consider a vector $\begin{bmatrix} -1&-1 \end{bmatrix}$ transforming to $\begin{bmatrix} -4&-3 \end{bmatrix}$ under a matrix.
- Normally, transformations alter a vector's direction, moving it off its span.
- Eigenvectors are exceptional; they stay on their span, simply scaling by the eigenvalue during transformation.
:::
:::

## Deep dive

::: columns
:::{.column}
![](img/eigenvector_messy.png)
![](img/eigenvector_scale.png)
:::
:::{.column .small}
### Eigenvector and Eigenvalue Complexity

- Eigenvectors and eigenvalues often involve complex, non-integer values, making precise calculation demanding.
- Scaling an original eigenvector retains its directionality, producing another scaled eigenvector.
:::
:::


## Deep dive

::: columns
::: {.column width="40%"}
![](img/3x3sqmatrix.png)
:::
::: {.column .small width="60%"}
### Eigenvalues and Eigenvectors in 3D Transformations

- In 3D, a matrix transforms the x, y, and z axes, each axis adjustment representing the transformation's effect.
- This specific relationship means eigenvectors and eigenvalues are exclusive to square matrices.
- Finding an eigenvector involves identifying its corresponding eigenvalue, through the equation $Ax=\lambda x$, where $A$ is the matrix and $\lambda$ the eigenvalue.
- This equation underscores that multiplying an eigenvector by its matrix equates to scaling it by the eigenvalue.

:::
:::

## Recall {.salt}

A square matrix is a general $n$ by $n$ matrix describing the transformation of $n$ axes, each corresponding to a coordinate with $n$ elements

## Deep dive

::: columns
:::{.column}
![](img/matrixalgebra.png)
:::
:::{.column .small}
### Solving for Eigenvalues

- To solve for eigenvalues, rearrange the equation to isolate terms on one side, introducing an identity matrix $I$ to equate matrix $A$ with scalar $\lambda$.
- Infinite "trivial solutions" exist, achievable by scaling an eigenvector by any scalar.
- To eliminate these trivial solutions, we utilize the determinant.
:::
:::

## Deep dive

::: columns
::: {.column}
![Figure 1](img/squareunit.png)
![Figure 2](img/4squnits.png)
:::
::: {.column .small}
### Understanding the Determinant

- The determinant quantifies how a transformation matrix stretches or compresses area.
- Consider a unit square in coordinate space (top figure). After transformation by $\begin{bmatrix} 2&1\\0 &2\end{bmatrix}$, its area expands to four square units (bottom figure.
- This increase indicates the determinant of the matrix is four, reflecting the area's stretching factor.
:::
:::

## Deep dive

::: columns
::: {.column}
![](img/det.png)
:::
::: {.column .small}
### Determinant and Space Collapse
- A determinant of zero means the transformation collapses the square's area to zero, indicating the axes' vectors align on the same line.
- With a zero determinant, space is condensed into a line, becoming one-dimensional.
- Thus, for the equation to be solvable without trivial solutions, the matrix's determinant must equal zero.
:::
:::

## Deep dive

::: columns
::: {.column}
![](img/detcalc.png)
:::
:::{.column .small}
### Finding Eigenvalues in 2D

- In 2D, determining eigenvalues involves solving a quadratic equation.
- For the matrix $\begin{bmatrix}1&4\\3&2\end{bmatrix}$, the eigenvalues are 5 and -2.
- Thus, multiplying the matrix's eigenvectors stretches their lengths by factors of 5 and -2, respectively.
:::
:::

## Three dimensional data eigenvector visualisation

![](img/3d.gif)

-   For three or more dimension matrix, a different form of the determinant formula must be used

## Deep dive
::: columns
:::{.column}
![](img/detcalc1.png)
:::
::: {.column .small}
### Retrieve the eigenvector

- To find a matrix's eigenvectors, insert each eigenvalue into the equation $Ax = \lambda x$. For $\lambda=5$, the eigenvector is $\begin{bmatrix}1\\1\end{bmatrix}$; for $\lambda=-2$, it's $\begin{bmatrix}-4\\3\end{bmatrix}$. 

- These eigenvectors and eigenvalues can reconstruct the original matrix, highlighting their crucial role in financial data science for simplifying and interpreting data transformations.

:::
:::

## Simplifying Complexity {.smaller}

#### Principal Component Analysis (PCA) in AI
- PCA, an essential technique in AI, focuses on reducing data dimensionality while preserving key statistical properties like variance and mean.
- Ideal for financial datasets with numerous features, PCA can transform a 100-dimensional dataset into a more manageable 2D version.
- The technique starts with the covariance matrix to evaluate variable correlations, defining the data's **shape**

:::{.saltinline}
A key concept we'll explore further in coding applications.
:::

## Simplifying Complexity {.smaller}

#### Unveiling Data's True Nature with Eigenvectors
- Eigenvectors expose dominant variance axes, highlighting crucial features in the dataset.
- Serving as dataset snapshots, eigenvectors guide feature modification, bolstering interpretability.
- Valuable in AI and finance, these mathematical objects extend beyond machine learning, deriving comprehensible insights from convoluted datasets.
- Exceptional property: consistent directionality persists after spatial transformations, laying bare the inner workings of matrices.

## Random Matrix Theory
- Random Matrix Theory offers a sophisticated method to separate noise from signal in empirical correlation matrices.
- By analyzing the distribution of eigenvalues through the Marcenko-Pastur law, we can identify and eliminate random noise.
- The goal is to distinguish noise-related eigenvalues from those signaling true data patterns.

## Denoising's role in finance
- In finance, accurate covariance matrices are crucial for algorithms that optimize portfolio liquidation by balancing risk and impact cost.
- Small or zero eigenvalues often indicate estimation errors due to inadequate data, compromising portfolio risk assessments.
- The random matrix technique helps mitigate the issue of small eigenvalues, enhancing model reliability.

## Understanding Random Correlation Matrices
- For $M$ stock returns over $T$ periods, the empirical correlation matrix $E$ can be represented as $E=\frac{1}{T} \sum x_{it}x_{jt}$, where $x_{it}$ is the normalized return of stock $i$.
- This matrix, expressed as $\bf{E=HH^{'}}$, encapsulates the correlation among stock returns.

## Eigenvalue Spectrum Analysis
- In the scenario where returns are random with variance $\sigma^2$, and with $T, M \rightarrow \infty$ keeping $Q:=T/M \ge 1$ constant, the eigenvalue density $\rho(\lambda)$ of $E$ follows a specific distribution.
- This distribution has bounds defined by maximum and minimum expected eigenvalues, $\lambda_{+}$ and $\lambda_{-}$, allowing the identification of significant eigenvalues within a sea of randomness.

## Denoising with Random Matrix Theory {.smaller}

### Generating and Denoising Synthetic Data 
- Generate a synthetic dataset of 100 stock returns over 500 days, assuming normal distribution.
```{r}
# Generate synthetic data
t <- 500  # Number of days
m <- 100  # Number of stocks
h <- array(rnorm(m*t), c(m, t))  # Random normal returns
e <- h %*% t(h) / t  # Correlation matrix
lambda_e <- eigen(e, symmetric = TRUE, only.values = TRUE)  # Eigenvalues and Eigenvector
ee<- lambda_e$values # extract the values
```


- **Note**: This process creates a large object; for testing on smaller systems, reduce `t` and `m`.

## Marcenko-Pastur Law for Denoising
- Calculate the Marcenko-Pastur probability density function (pdf) to separate noise from signal.
```{r}
# Marcenko-Pastur pdf calculation
library(matlab)
mp_pdf <- function(var, t, m, pts) {
  q <- t / m
  eMin <- var * (1 - (1 / q)^0.5)^2
  eMax <- var * (1 + (1 / q)^0.5)^2
  eVal <- linspace(eMin, eMax, pts)
  pdf <- q / (2 * pi * var * eVal) * ((eMax - eVal) * (eVal - eMin))^0.5
  return(setNames(array(pdf), eVal))
}
```


## Visualizing Denoising Efficacy

::: columns
::: {.column .small}
- Visualize how well RMT approximates real data distributions through empirical and theoretical density plots.
```{r}
#| eval: false
#| echo: true
# Visualize Marcenko-Pastur vs. empirical distribution
library(ggplot2)
empirical_pdf <- density(ee, width = 0.1, kernel = "gaussian")
marcenko_pastur_pdf <- mp_pdf(1, t, m, m)
ggplot() +
  geom_line(data = tibble(eval = empirical_pdf$x, 
                          pdf = empirical_pdf$y, 
                          type = "Empirical PDF"), aes(x = eval, y = pdf, colour = type)) +
  geom_line(data = tibble(eval = names(marcenko_pastur_pdf), 
                          pdf = marcenko_pastur_pdf, type = "Marcenko-Pastur PDF"), 
            aes(x = as.numeric(eval), y = pdf, colour = type)) +
  labs(title = "Marcenko-Pastur Distribution vs. Empirical Data", x = "Eigenvalues", y = "Density") +
  theme(legend.title = element_blank())
```
:::
::: {.column}
```{r}
#| eval: true
#| echo: false

# Visualize Marcenko-Pastur vs. empirical distribution
library(ggplot2)
empirical_pdf <- density(ee, width = 0.1, kernel = "gaussian")
marcenko_pastur_pdf <- mp_pdf(1, t, m, m)
ggplot() +
  geom_line(data = tibble(eval = empirical_pdf$x, 
                          pdf = empirical_pdf$y, 
                          type = "Empirical PDF"), aes(x = eval, y = pdf, colour = type)) +
  geom_line(data = tibble(eval = names(marcenko_pastur_pdf), 
                          pdf = marcenko_pastur_pdf, type = "Marcenko-Pastur PDF"), 
            aes(x = as.numeric(eval), y = pdf, colour = type)) +
  labs(title = "Marcenko-Pastur Distribution vs. Empirical Data", x = "Eigenvalues", y = "Density") +
  theme(legend.title = element_blank())
```
:::
:::

## Applying RMT to Real Financial Data
- Apply RMT to Dow Jones 30 daily returns to optimize the covariance matrix.
```{r}
# Load data and apply RMT
load("dow30data.RData")
model <- estRMT(dow30data, parallel = FALSE)  # estRMT function optimizes the covariance matrix
```

- `fml::estRMT()` is part of the R package for this course details of which can be found <https://github/quinfer/fml>

## Portfolio Optimization Using Denoised Data {.smaller}

-  Optimising a portfolio with denoised data involves adjusting constraints and objectives with the `PortfolioAnalytics` R framework to reflect the refined risk and return estimates

```{r}
# Load the PortfolioAnalytics library, which provides tools for portfolio analysis
library(PortfolioAnalytics)

# Initialize a portfolio specification object with the assets derived from the column names of the dow30data dataset.
pspec.lo <- portfolio.spec(assets = colnames(dow30data))

# Add a full investment constraint mandating that the sum of the asset weights equals 100%, enforcing full investment of available capital.
pspec.lo <- add.constraint(pspec.lo, type = "full_investment")

# This ensures that the portfolio can only take long positions in assets, prohibiting short selling.
pspec.lo <- add.constraint(pspec.lo, type = "long_only")

# Set up objectives for mean-variance optimization: maximize mean return and minimize portfolio variance (risk)
pspec.lo <- add.objective(portfolio = pspec.lo, type = "return", name = "mean")
pspec.lo <- add.objective(portfolio = pspec.lo, type = "risk", name = "var")
```

## Explanation

- This block of code demonstrates the initial steps in setting up a portfolio optimization problem using denoised data. 
- By specifying constraints and objectives, it lays the groundwork for finding an optimal asset allocation that balances the dual goals of maximizing returns while minimizing risk, based on the refined covariance matrix obtained through denoising techniques such as Random Matrix Theory.


## Custom momentum function

- Let us first construct a custom moment function where covariance is built by denoising using Random Matrix Theory. We assume no third/fourth order effects.

```{r}
custom.portfolio.moments <- function(R, portfolio) {
  momentargs<-list()
  momentargs$mu<-matrix(as.vector(apply(R,2, "mean")), ncol = 1)
  momentargs$sigma<-estRMT(R, parallel=FALSE)$cov
   momentargs$m3 <- matrix(0, nrow=ncol(R), ncol=ncol(R)^2)
  momentargs$m4 <- matrix(0, nrow=ncol(R), ncol=ncol(R)^3)
  return(momentargs)
}
```

## Backtest strategy

- Now lets backtest our strategy using an ordinary covariance matrix and a covariance matrix build by denoising using Random Matrix theory.

```{r}
opt.ordinary <- optimize.portfolio.rebalancing(
  dow30data,pspec.lo, 
  optimize_method="ROI",
  rebalance_on='months',
  training_period=30,
  trailing_periods=30)
opt.rmt <-optimize.portfolio.rebalancing(
  dow30data, pspec.lo,
  optimize_method="ROI",
  momentFUN = "custom.portfolio.moments",
  rebalance_on="months",
  training_period=30,
  trailing_periods=30)
```

## Extract the optimal weight from backtests
- We can now extract weights and build cumulative returns using the 
PerformanceAnalytics
 package.
```{r}
ordinary.wts <- na.omit(extractWeights(opt.ordinary))
ordinary <- Return.rebalancing(R=dow30data, weights=ordinary.wts)
rmt.wts <- na.omit(extractWeights(opt.rmt))
rmt <- Return.rebalancing(R=dow30data, weights=rmt.wts)
rmt.strat.rets <- merge.xts(ordinary,rmt)
colnames(rmt.strat.rets) <- c("ordinary", "rmt")
```


## Benefits of denoising

```{r}
charts.PerformanceSummary(rmt.strat.rets, 
                          colorset = c("red", "darkgrey"), 
                          main = "Advantages of Denoising in Portfolio Management", 
                          legend.loc = "topleft")
chart.RiskReturnScatter(rmt.strat.rets)
```


## Portfolio Performance with Denoising {.smaller}

- The success of RMT in filtering out noise underscores the importance of sophisticated data analysis techniques in modern finance. 
- By enabling a clearer understanding of underlying market dynamics, RMT facilitates more informed and strategic decision-making processes.
- Implementing denoising techniques, particularly Random Matrix Theory (RMT), markedly improves portfolio performance. 
- This method effectively separates genuine market signals from noise, leading to more accurate risk assessments and investment strategies.


## Reducing Drawdowns through Advanced Data Analysis

- The application of RMT not only boosts returns but also minimises drawdowns, thereby reducing the potential downside risk associated with investment strategies. 
- This demonstrates the technique's capability to enhance the stability and resilience of portfolios against market volatility.

## Empirical Evidence Supporting Denoising

- Comparative analysis of strategies using ordinary covariance matrices versus those refined through RMT denoising reveals a clear advantage for the latter. 
- The empirical results highlight not just improved returns but also a significant decrease in investment risk.



## Wrapping Up
#### Denoising Financial Data: Insights from the Marcenko-Pastur Law

- **Marcenko-Pastur Law Fundamentals**:
  - Describes the distribution of eigenvalues of large sample covariance matrices.
  - Applies when both the number of variables and observations are large, with their ratio converging to a constant.
  - Helps in distinguishing signal from noise in high-dimensional datasets.

## Wrapping Up

- **Applications in AI & Machine Learning**:
  - Critical for feature selection and dimensionality reduction (e.g., in PCA).
  - Enables identification of significant components, enhancing model accuracy.
  - Facilitates efficient data processing and analysis in AI algorithms.

## Wrapping Up
- **Implications for Trading & Financial Analysis**
  - Assists in portfolio optimization by distinguishing genuine asset correlations from random noise.
  - Enhances risk management by providing a clearer understanding of the structure of financial data.
  - Supports the development of sophisticated quantitative trading strategies, focusing on robustness and risk reduction.

## Key Takeaways {.saltinline}
  - The Marcenko-Pastur law offers a theoretical foundation for analyzing and denoising high-dimensional financial datasets.
  - Understanding the eigenvalue distribution of covariance matrices is crucial for extracting meaningful information in both AI applications and financial trading.
  - Leveraging this law can lead to more informed decision-making, improved model performance, and enhanced trading results.

