---
title: "Denoising financial data"
subtitle: "And its effect of trading strategies"
author: "Barry Quinn"
date: "`r Sys.Date()`"
css: mycssblend.css
logo: img/qbslogo.png
title-slide-attributes:
  data-background-image: img/title-slide-img.png
  data-background-size: contain
  data-background-opacity: "0.5"
footer: "AI and Trading"
execute: 
  echo: fenced
format: 
  revealjs:
    scrollable: true
    slide-number: c/t
---

```{r setup, include=FALSE}
library(tidyverse)
library(babynames)
library(fontawesome) 
library(DiagrammeR)
library(RefManageR)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "alphabetic",
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("refs.bib", check = FALSE)
source("fml_fns.R")
```

## Learning outcomes

-   Growth learning of mathematics
-   The Jewel of the Matrix: Eigenvalues and Eigenvectors
-   Random Matrix Theory technique: **Marcenko-Pastur Theorem**
-   Denoising financial covariance matrices
-   Real world application for investment algorithms

## Enhancing Mathematical Understanding in Machine Learning {.small}

- Machine learning's essence lies in grasping numerous mathematical foundations.
- Our course aims to nurture an intuitive grasp of these principles, steering clear of excessive technicality.
- We acknowledge that solely relying on mathematical notation might deter engagement.
- To counter this, we'll adopt a dynamic learning strategy, employing code, discussions, and tangible examples to bring concepts to life.
- Embracing a multifaceted learning methodology not only fosters deeper understanding but also ensures the retention of knowledge.
- Predominantly, our sessions will feature hands-on coding exercises. This "learn by doing" philosophy encourages experimentation, allowing for mistakes that solidify learning.

## Why study denoising and detoning? {.small}

::: columns
::: {.column}
-   Covariance matrices are everywhere in finance
-   Empirical covariances measure the linear co-movement between a set of random variables
-   For example to estimate the linear comovement between FTSE 100 stocks you would gather 100 time series of each stocks returns

::: 
:::{.column}
-   They are used to:

1.  Run regressions
2.  Estimate risk
3.  **Optimise portfolios**
4.  Simulate scenarios via Monte Carlo
5.  Find clusters
6.  Reduce the dimensionality of a set of potential predictors
:::
:::

## Eigen.....

#### The story of the billion dollar eigenvector

-   Larry page and Sergy Bin built a billion dollar company based on solving a linear alegbra problem using eigenvectors
-   In the following clip, Professor Steve Strogatz tells the story of the eigenvector, and its billion dollar application in Google.

[Soundcloud clip](https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/302257938&color=%23d0aca4&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true&visual=true)

## Core Insights in Financial Matrices {.smaller}

#### **Eigenvalues and Eigenvectors: The Essence of Big Data in Finance**

- The shift towards Big Data is reshaping finance, emphasizing the need for data-driven insights.
- Data are increasingly structured in matrices, central to modern financial analysis.
- Success in quantitative finance now hinges on the ability to decode complex data matrices and convey their essence.
- At the heart of these matrices are eigenvalues and eigenvectors, key to unlocking clear, actionable insights from within the data maze.
- Understanding these concepts is crucial for navigating the data-centric landscape of contemporary finance.

## Deep dive

::: columns
::: {.column}
![](img/vectors.png)
:::

::: {.column .small}
#### Simplifying Matrix Concepts: The 2D Vector
- A foundational element in matrix theory, the two-dimensional vector, embodies simplicity.
- Comprising just two components, it directly maps to coordinates on a 2D plane.
- This fundamental structure serves as a practical example of how matrices relate to geometric spaces.
:::
:::

## Deep dive

::: columns
::: {.column}

![](img/lineartransform.png)
:::
::: {.column .small}
### Linear Transformation Essentials

- Linear transformations adjust vectors through matrix multiplication, stretching or compressing them along defined axes.
- The matrix $\begin{bmatrix} 3 & 1 \\ 1 & 2 \end{bmatrix}$ shifts the x-axis to align with $\begin{bmatrix} 3&1 \end{bmatrix}$ and the y-axis with $\begin{bmatrix} 1&2 \end{bmatrix}$.
- These shifts are visualized by color-coded lines, showing how the transformation alters the coordinate system's orientation.
:::
:::

## Deep dive

::: columns
::: {.column}
![](img/lineartransform2.png)
:::
::: {.column .small}
#### Eigenvectors: Stability in Transformation

- Consider a vector $\begin{bmatrix} -1&-1 \end{bmatrix}$ transforming to $\begin{bmatrix} -4&-3 \end{bmatrix}$ under a matrix.
- Normally, transformations alter a vector's direction, moving it off its span.
- Eigenvectors are exceptional; they stay on their span, simply scaling by the eigenvalue during transformation.
:::
:::

## Deep dive

::: columns
:::{.column}
![](img/eigenvector_messy.png)
![](img/eigenvector_scale.png)
:::
:::{.column .small}
### Eigenvector and Eigenvalue Complexity

- Eigenvectors and eigenvalues often involve complex, non-integer values, making precise calculation demanding.
- Scaling an original eigenvector retains its directionality, producing another scaled eigenvector.
:::
:::


## Deep dive

::: columns
::: {.column width="40%"}
![](img/3x3sqmatrix.png)
:::
::: {.column .small width="60%"}
### Eigenvalues and Eigenvectors in 3D Transformations

- In 3D, a matrix transforms the x, y, and z axes, each axis adjustment representing the transformation's effect.
- This specific relationship means eigenvectors and eigenvalues are exclusive to square matrices.
- Finding an eigenvector involves identifying its corresponding eigenvalue, through the equation $Ax=\lambda x$, where $A$ is the matrix and $\lambda$ the eigenvalue.
- This equation underscores that multiplying an eigenvector by its matrix equates to scaling it by the eigenvalue.

:::
:::

## Recall {.salt}

A square matrix is a general $n$ by $n$ matrix describing the transformation of $n$ axes, each corresponding to a coordinate with $n$ elements

## Deep dive

::: columns
:::{.column}
![](img/matrixalgebra.png)
:::
:::{.column .small}
### Solving for Eigenvalues

- To solve for eigenvalues, rearrange the equation to isolate terms on one side, introducing an identity matrix $I$ to equate matrix $A$ with scalar $\lambda$.
- Infinite "trivial solutions" exist, achievable by scaling an eigenvector by any scalar.
- To eliminate these trivial solutions, we utilize the determinant.
:::
:::

## Deep dive

::: columns
::: {.column}
![Figure 1](img/squareunit.png)
![Figure 2](img/4squnits.png)
:::
::: {.column .small}
### Understanding the Determinant

- The determinant quantifies how a transformation matrix stretches or compresses area.
- Consider a unit square in coordinate space (top figure). After transformation by $\begin{bmatrix} 2&1\\0 &2\end{bmatrix}$, its area expands to four square units (bottom figure.
- This increase indicates the determinant of the matrix is four, reflecting the area's stretching factor.
:::
:::

## Deep dive

::: columns
::: {.column}
![](img/det.png)
:::
::: {.column .small}
### Determinant and Space Collapse
- A determinant of zero means the transformation collapses the square's area to zero, indicating the axes' vectors align on the same line.
- With a zero determinant, space is condensed into a line, becoming one-dimensional.
- Thus, for the equation to be solvable without trivial solutions, the matrix's determinant must equal zero.
:::
:::

## Deep dive

::: columns
::: {.column}
![](img/detcalc.png)
:::
:::{.column .small}
### Finding Eigenvalues in 2D

- In 2D, determining eigenvalues involves solving a quadratic equation.
- For the matrix $\begin{bmatrix}1&4\\3&2\end{bmatrix}$, the eigenvalues are 5 and -2.
- Thus, multiplying the matrix's eigenvectors stretches their lengths by factors of 5 and -2, respectively.
:::
:::

## Three dimensional data eigenvector visualisation

![](img/3d.gif)

-   For three or more dimension matrix, a different form of the determinant formula must be used

## Deep dive
::: columns
:::{.column}
![](img/detcalc1.png)
:::
::: {.column .small}
- To find a matrix's eigenvectors, insert each eigenvalue into the equation $Ax = \lambda x$. For $\lambda=5$, the eigenvector is $\begin{bmatrix}1\\1\end{bmatrix}$; for $\lambda=-2$, it's $\begin{bmatrix}-4\\3\end{bmatrix}$. 

- These eigenvectors and eigenvalues can reconstruct the original matrix, highlighting their crucial role in financial data science for simplifying and interpreting data transformations.
:::
:::

## Simplifying Complexity in AI for Finance {.smaller}


#### Principal Component Analysis (PCA) in AI
- PCA, an essential technique in AI, focuses on reducing data dimensionality while preserving key statistical properties like variance and mean.
- Ideal for financial datasets with numerous features, PCA can transform a 100-dimensional dataset into a more manageable 2D version.
- The technique starts with the covariance matrix to evaluate variable correlations, defining the data's **shape**
:::{.saltinline}
A key concept we'll explore further in coding applications.
:::

## Simplifying Complexity in AI for Finance {.smaller}

#### Unveiling Data's True Nature with Eigenvectors
- Eigenvectors of the covariance matrix are crucial for reorienting data along axes of greatest variance, illuminating significant features.
- They provide a snapshot of the dataset, guiding which features to enhance or reduce, thereby optimizing data interpretation.

::: callout-important
- Beyond machine learning, eigenvectors and eigenvalues are celebrated for their ability to distill complex datasets into meaningful insights. 
- Their defining quality is their directionality consistency amidst spatial transformations, revealing the underlying essence of matrices—a testament to their indispensable role in AI and finance.
:::

## Why study denoising?

-   Empirical covariance matrices are estimated with flawed, incomplete data which leads to estimates with an amount of noise

-   Such noise can render calculations using covariance matrix estimates useless

-   In finance, we need a procedure to reduce this noise and enhance the signal **before** using in subsequent analysis like those listed previously

## Streamlining Denoising in Finance

### The Importance of Denoising
- Empirical covariance matrices often contain noise due to flawed or incomplete data.
- This noise can undermine the utility of covariance-based calculations in finance.
- A denoising procedure is essential for refining data prior to analysis.

## Denoising with Random Matrix Theory
- Random Matrix Theory offers a sophisticated method to separate noise from signal in empirical correlation matrices.
- By analyzing the distribution of eigenvalues through the Marcenko-Pastur law, we can identify and eliminate random noise.
- The goal is to distinguish noise-related eigenvalues from those signaling true data patterns.

## Denoising's Role in Financial Modelling
- In finance, accurate covariance matrices are crucial for algorithms that optimize portfolio liquidation by balancing risk and impact cost.
- Small or zero eigenvalues often indicate estimation errors due to inadequate data, compromising portfolio risk assessments.
- The random matrix technique helps mitigate the issue of small eigenvalues, enhancing model reliability.

## Understanding Random Correlation Matrices
- For $M$ stock returns over $T$ periods, the empirical correlation matrix $E$ can be represented as $E=\frac{1}{T} \sum x_{it}x_{jt}$, where $x_{it}$ is the normalized return of stock $i$.
- This matrix, expressed as $\bf{E=HH^{'}}$, encapsulates the correlation among stock returns.

## Eigenvalue Spectrum Analysis
- In the scenario where returns are random with variance $\sigma^2$, and with $T, M \rightarrow \infty$ keeping $Q:=T/M \ge 1$ constant, the eigenvalue density $\rho(\lambda)$ of $E$ follows a specific distribution.
- This distribution has bounds defined by maximum and minimum expected eigenvalues, $\lambda_{+}$ and $\lambda_{-}$, allowing the identification of significant eigenvalues within a sea of randomness.

## Streamlining Denoising in Finance with Random Matrix Theory

### Generating and Denoising Synthetic Data
- Generate a synthetic dataset of 100 stock returns over 500 days, assuming normal distribution.
```{r}
# Generate synthetic data
t <- 500  # Number of days
m <- 100  # Number of stocks
h <- array(rnorm(m*t), c(m, t))  # Random normal returns
e <- h %*% t(h) / t  # Correlation matrix
lambda_e <- eigen(e, symmetric = TRUE, only.values = TRUE)  # Eigenvalues and Eigenvector
ee<- lambda_e$values # extract the values
```
**Note**: This process creates a large object; for testing on smaller systems, reduce `t` and `m`.

## Marcenko-Pastur Law for Denoising
- Calculate the Marcenko-Pastur probability density function (pdf) to separate noise from signal.
```{r}
# Marcenko-Pastur pdf calculation
library(matlab)
mp_pdf <- function(var, t, m, pts) {
  q <- t / m
  eMin <- var * (1 - (1 / q)^0.5)^2
  eMax <- var * (1 + (1 / q)^0.5)^2
  eVal <- linspace(eMin, eMax, pts)
  pdf <- q / (2 * pi * var * eVal) * ((eMax - eVal) * (eVal - eMin))^0.5
  return(setNames(array(pdf), eVal))
}
```


## Visualizing Denoising Efficacy
- Visualize how well RMT approximates real data distributions through empirical and theoretical density plots.
```{r}
# Visualize Marcenko-Pastur vs. empirical distribution
library(ggplot2)
empirical_pdf <- density(ee, width = 0.1, kernel = "gaussian")
marcenko_pastur_pdf <- mp_pdf(1, t, m, m)
ggplot() +
  geom_line(data = tibble(eval = empirical_pdf$x, pdf = empirical_pdf$y, type = "Empirical PDF"), aes(x = eval, y = pdf, colour = type)) +
  geom_line(data = tibble(eval = names(marcenko_pastur_pdf), pdf = marcenko_pastur_pdf, type = "Marcenko-Pastur PDF"), aes(x = as.numeric(eval), y = pdf, colour = type)) +
  labs(title = "Marcenko-Pastur Distribution vs. Empirical Data", x = "Eigenvalues", y = "Density") +
  theme(legend.title = element_blank())
```

## Applying RMT to Real Financial Data
- Apply RMT to Dow Jones 30 daily returns to optimize the covariance matrix.
```{r}
# Load data and apply RMT
load("dow30data.RData")
model <- estRMT(dow30data, parallel = FALSE)  # estRMT function optimizes the covariance matrix
```

::: callout-important
`fml::estRMT()` is part of the R package for this course details of which can be found <https://github/quinfer/fml>
:::

## Portfolio Optimization Using Denoised Data

- - Optimising a portfolio with denoised data involves adjusting constraints and objectives with the `PortfolioAnalytics` framework to reflect the refined risk and return estimates

```{r}
# Load the PortfolioAnalytics library, which provides tools for portfolio analysis
library(PortfolioAnalytics)

# Initialize a portfolio specification object with the assets derived from the column names of the dow30data dataset.
# This step sets up the portfolio with the specified assets, preparing it for further configuration.
pspec.lo <- portfolio.spec(assets = colnames(dow30data))

# Add a full investment constraint to ensure that all capital is allocated across the portfolio's assets.
# This constraint mandates that the sum of the asset weights equals 100%, enforcing full investment of available capital.
pspec.lo <- add.constraint(pspec.lo, type = "full_investment")

# Add a long-only constraint to restrict portfolio positions to be non-negative.
# This ensures that the portfolio can only take long positions in assets, prohibiting short selling.
pspec.lo <- add.constraint(pspec.lo, type = "long_only")

# Define the portfolio's objective to maximize the mean return.
# This objective instructs the optimization algorithm to seek the highest possible expected return from the portfolio.
pspec.lo <- add.objective(portfolio = pspec.lo, type = "return", name = "mean")

# Add another objective to minimize the portfolio's variance, targeting risk reduction.
# This objective aims to construct a portfolio that achieves the lowest possible risk, as measured by the variance of portfolio returns.
pspec.lo <- add.objective(portfolio = pspec.lo, type = "risk", name = "var")
```

## Explanation

- This block of code demonstrates the initial steps in setting up a portfolio optimization problem using denoised data. 
- By specifying constraints and objectives, it lays the groundwork for finding an optimal asset allocation that balances the dual goals of maximizing returns while minimizing risk, based on the refined covariance matrix obtained through denoising techniques such as Random Matrix Theory.


## Custom momentum function

- Let us first construct a custom moment function where covariance is built by denoising using Random Matrix Theory. We assume no third/fourth order effects.

```{r}
custom.portfolio.moments <- function(R, portfolio) {
  momentargs<-list()
  momentargs$mu<-matrix(as.vector(apply(R,2, "mean")), ncol = 1)
  momentargs$sigma<-estRMT(R, parallel=FALSE)$cov
   momentargs$m3 <- matrix(0, nrow=ncol(R), ncol=ncol(R)^2)
  momentargs$m4 <- matrix(0, nrow=ncol(R), ncol=ncol(R)^3)
  return(momentargs)
}
```

## Backtest strategy

- Now lets backtest our strategy using an ordinary covariance matrix and a covariance matrix build by denoising using Random Matrix theory.

```{r}
opt.ordinary <- optimize.portfolio.rebalancing(
  dow30data,pspec.lo, 
  optimize_method="ROI",
  rebalance_on='months',
  training_period=30,
  trailing_periods=30)
opt.rmt <-optimize.portfolio.rebalancing(
  dow30data, pspec.lo,
  optimize_method="ROI",
  momentFUN = "custom.portfolio.moments",
  rebalance_on="months",
  training_period=30,
  trailing_periods=30)
```

## Extract the optimal weight from backtests
- We can now extract weights and build cumulative returns using the 
PerformanceAnalytics
 package.
```{r}
ordinary.wts <- na.omit(extractWeights(opt.ordinary))
ordinary <- Return.rebalancing(R=dow30data, weights=ordinary.wts)
rmt.wts <- na.omit(extractWeights(opt.rmt))
rmt <- Return.rebalancing(R=dow30data, weights=rmt.wts)
rmt.strat.rets <- merge.xts(ordinary,rmt)
colnames(rmt.strat.rets) <- c("ordinary", "rmt")
```


## The Value of Noise Filtering in Financial Decision-Making

::: columns
::: {.column}
- The success of RMT in filtering out noise underscores the importance of sophisticated data analysis techniques in modern finance. 
- By enabling a clearer understanding of underlying market dynamics, RMT facilitates more informed and strategic decision-making processes.
:::
::: {.column}
```{r}

charts.PerformanceSummary(rmt.strat.rets, wealth.index = TRUE, 
                          colorset = c("red", "darkgrey"), 
                          main = "Advantages of Denoising in Portfolio Management", 
                          legend.loc = "topleft")
```
:::
:::

## Enhancing Portfolio Performance with Denoising

- Implementing denoising techniques, particularly Random Matrix Theory (RMT), markedly improves portfolio performance. 
- This method effectively separates genuine market signals from noise, leading to more accurate risk assessments and investment strategies.


## Reducing Drawdowns through Advanced Data Analysis

- The application of RMT not only boosts returns but also minimises drawdowns, thereby reducing the potential downside risk associated with investment strategies. 
- This demonstrates the technique's capability to enhance the stability and resilience of portfolios against market volatility.

## Empirical Evidence Supporting Denoising

- Comparative analysis of strategies using ordinary covariance matrices versus those refined through RMT denoising reveals a clear advantage for the latter. 
- The empirical results highlight not just improved returns but also a significant decrease in investment risk.


## Wrapping Up {.smaller}

- The process of denoising, especially with techniques such as Random Matrix Theory, significantly improves the quality of financial data analysis. 
- By filtering out noise, RMT allows for a clearer understanding of the underlying market dynamics, which is crucial for making informed investment decisions and developing robust financial models.
- The practical implementation of these techniques demonstrates their value in real-world applications, providing finance professionals with powerful tools to enhance portfolio performance and reduce risk. 
- Through detailed code examples and explanations, we've seen how RMT can be applied to both synthetic and real financial datasets to extract meaningful insights and optimize investment strategies.
